{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNixuYtHQlGd6vfrKQ6JV4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarmi2325/NLP/blob/main/Frequency_based_Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qiSwMH8IpJs3"
      },
      "outputs": [],
      "source": [
        "sample_text = [\"Self-love, in the context of AI or otherwise, is about developing a positive and accepting attitude towards oneself, recognizing one's worth, and prioritizing one's well-being.\",\n",
        "               \"It involves treating oneself with kindness, compassion, and respect, while acknowledging both strengths and weaknesses.\",\n",
        "               \" Self-love is not about arrogance or narcissism, but rather a foundation for healthy relationships, resilience, and overall happiness.\",\n",
        "               \"It's about accepting your flaws, celebrating your strengths, and taking care of your physical and mental health.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BAG OF WORDS**"
      ],
      "metadata": {
        "id": "NnbluAoYqAyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# creates a vocabulary and for each sentence it will give the frequency of each word\n",
        "# high frequency gets high-weight and low-frequency gets low-weights (disadvantage because stop words occur many times)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sample_text)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na9vFpxkp90h",
        "outputId": "6dd21042-6134-48ee-8ae8-d86e4f2731cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about' 'accepting' 'acknowledging' 'ai' 'and' 'arrogance' 'attitude'\n",
            " 'being' 'both' 'but' 'care' 'celebrating' 'compassion' 'context'\n",
            " 'developing' 'flaws' 'for' 'foundation' 'happiness' 'health' 'healthy'\n",
            " 'in' 'involves' 'is' 'it' 'kindness' 'love' 'mental' 'narcissism' 'not'\n",
            " 'of' 'one' 'oneself' 'or' 'otherwise' 'overall' 'physical' 'positive'\n",
            " 'prioritizing' 'rather' 'recognizing' 'relationships' 'resilience'\n",
            " 'respect' 'self' 'strengths' 'taking' 'the' 'towards' 'treating'\n",
            " 'weaknesses' 'well' 'while' 'with' 'worth' 'your']\n",
            "[[1 1 0 1 2 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 2 1 1 1 0\n",
            "  0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0]\n",
            " [0 0 1 0 2 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0]\n",
            " [1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1\n",
            "  0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 2 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TERM FREQUENCY**"
      ],
      "metadata": {
        "id": "dJTham-HrUfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the BoW\n",
        "#no.of freq of term/ total terms\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "trans = TfidfTransformer(use_idf=False)\n",
        "X = trans.fit_transform(X)\n",
        "\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giw-8_zHrZTv",
        "outputId": "7dda3ae2-0e26-42c2-8c02-f86b5c17202d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.13070481 0.16144644 0.         0.20477434 0.21371949 0.\n",
            "  0.20477434 0.20477434 0.         0.         0.         0.\n",
            "  0.         0.20477434 0.20477434 0.         0.         0.\n",
            "  0.         0.         0.         0.20477434 0.         0.16144644\n",
            "  0.         0.         0.16144644 0.         0.         0.\n",
            "  0.16144644 0.40954868 0.16144644 0.16144644 0.20477434 0.\n",
            "  0.         0.20477434 0.20477434 0.         0.20477434 0.\n",
            "  0.         0.         0.16144644 0.         0.         0.20477434\n",
            "  0.20477434 0.         0.         0.20477434 0.         0.\n",
            "  0.20477434 0.        ]\n",
            " [0.         0.         0.27784154 0.         0.28997848 0.\n",
            "  0.         0.         0.27784154 0.         0.         0.\n",
            "  0.27784154 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.27784154 0.\n",
            "  0.21905346 0.27784154 0.         0.         0.         0.\n",
            "  0.         0.         0.21905346 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.27784154 0.         0.21905346 0.         0.\n",
            "  0.         0.27784154 0.27784154 0.         0.27784154 0.27784154\n",
            "  0.         0.        ]\n",
            " [0.16390005 0.         0.         0.         0.13399903 0.2567811\n",
            "  0.         0.         0.         0.2567811  0.         0.\n",
            "  0.         0.         0.         0.         0.2567811  0.2567811\n",
            "  0.2567811  0.         0.2567811  0.         0.         0.20244917\n",
            "  0.         0.         0.20244917 0.         0.2567811  0.2567811\n",
            "  0.         0.         0.         0.20244917 0.         0.2567811\n",
            "  0.         0.         0.         0.2567811  0.         0.2567811\n",
            "  0.2567811  0.         0.20244917 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.14278582 0.1763689  0.         0.         0.23347353 0.\n",
            "  0.         0.         0.         0.         0.22370158 0.22370158\n",
            "  0.         0.         0.         0.22370158 0.         0.\n",
            "  0.         0.22370158 0.         0.         0.         0.\n",
            "  0.1763689  0.         0.         0.22370158 0.         0.\n",
            "  0.1763689  0.         0.         0.         0.         0.\n",
            "  0.22370158 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.1763689  0.22370158 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.67110475]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TERM FREQUENC- INVERSE DOCUMENT FREQUENCY**"
      ],
      "metadata": {
        "id": "0YJibDBmtsxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#this technique give high weights to low frequency and low weights to high frequency\n",
        "# tf*log(t/IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X=vectorizer.fit_transform(sample_text)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5dtg7MMtzDg",
        "outputId": "3773dec1-06fc-4c41-d412-fac66994811b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about' 'accepting' 'acknowledging' 'ai' 'and' 'arrogance' 'attitude'\n",
            " 'being' 'both' 'but' 'care' 'celebrating' 'compassion' 'context'\n",
            " 'developing' 'flaws' 'for' 'foundation' 'happiness' 'health' 'healthy'\n",
            " 'in' 'involves' 'is' 'it' 'kindness' 'love' 'mental' 'narcissism' 'not'\n",
            " 'of' 'one' 'oneself' 'or' 'otherwise' 'overall' 'physical' 'positive'\n",
            " 'prioritizing' 'rather' 'recognizing' 'relationships' 'resilience'\n",
            " 'respect' 'self' 'strengths' 'taking' 'the' 'towards' 'treating'\n",
            " 'weaknesses' 'well' 'while' 'with' 'worth' 'your']\n",
            "[[0.13070481 0.16144644 0.         0.20477434 0.21371949 0.\n",
            "  0.20477434 0.20477434 0.         0.         0.         0.\n",
            "  0.         0.20477434 0.20477434 0.         0.         0.\n",
            "  0.         0.         0.         0.20477434 0.         0.16144644\n",
            "  0.         0.         0.16144644 0.         0.         0.\n",
            "  0.16144644 0.40954868 0.16144644 0.16144644 0.20477434 0.\n",
            "  0.         0.20477434 0.20477434 0.         0.20477434 0.\n",
            "  0.         0.         0.16144644 0.         0.         0.20477434\n",
            "  0.20477434 0.         0.         0.20477434 0.         0.\n",
            "  0.20477434 0.        ]\n",
            " [0.         0.         0.27784154 0.         0.28997848 0.\n",
            "  0.         0.         0.27784154 0.         0.         0.\n",
            "  0.27784154 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.27784154 0.\n",
            "  0.21905346 0.27784154 0.         0.         0.         0.\n",
            "  0.         0.         0.21905346 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.27784154 0.         0.21905346 0.         0.\n",
            "  0.         0.27784154 0.27784154 0.         0.27784154 0.27784154\n",
            "  0.         0.        ]\n",
            " [0.16390005 0.         0.         0.         0.13399903 0.2567811\n",
            "  0.         0.         0.         0.2567811  0.         0.\n",
            "  0.         0.         0.         0.         0.2567811  0.2567811\n",
            "  0.2567811  0.         0.2567811  0.         0.         0.20244917\n",
            "  0.         0.         0.20244917 0.         0.2567811  0.2567811\n",
            "  0.         0.         0.         0.20244917 0.         0.2567811\n",
            "  0.         0.         0.         0.2567811  0.         0.2567811\n",
            "  0.2567811  0.         0.20244917 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.14278582 0.1763689  0.         0.         0.23347353 0.\n",
            "  0.         0.         0.         0.         0.22370158 0.22370158\n",
            "  0.         0.         0.         0.22370158 0.         0.\n",
            "  0.         0.22370158 0.         0.         0.         0.\n",
            "  0.1763689  0.         0.         0.22370158 0.         0.\n",
            "  0.1763689  0.         0.         0.         0.         0.\n",
            "  0.22370158 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.1763689  0.22370158 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.67110475]]\n"
          ]
        }
      ]
    }
  ]
}